# 문서
## 문서 유사도
- 추천 시스템
- 검색 및 정보 탐색
- 분류, 군집 및 중복 검출
- 의사결정 지원

## 문서 유사도 수치화 방법
- 자카드 유사도
    - 문서를 '단어의 집합'으로 보고, 공통으로 들어있는 '단어의 비율'을 계산 (공통 단어/전체 단어)
- 코사인 유사도
    - 문서를 벡터로 표현한 뒤, 두 벡터가 이루는 '각도의 코사인 값'
    - -1 이상 1 이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단 (방향)
    - 얼마나 같은 방향인지 확인
- 유클리드 거리
    - 문서를 벡터로 표현한 뒤, 두 벡터 사이의 '직선 거리'를 계산
    - 거리가 0에 가까울수록 유사도가 높다고 판단 (거리)

## 코사인 유사도
- '방향과 크기'를 나타내는 수학적 표현
- 1차원 벡터, [v]
    - 실수선에서 한 점을 가리키는 화살표
- 2차원 벡터, [x, y]
    - 2D 평면에서 '오른쪽x, 위로y' 을 가리키는 화살표
- 3차원 벡터, [x,y,z]
    - 3D 평면에서 'x축,y축,z축'을 가리키는 화살표
- 고차원 벡터, [v1,v2 ... vn]
    - N개의 실수 성분의 '크기와 방향'
    - 문서에 1000개 단어 있으면 1000차원 벡터가 문서를 나타낸다고 볼 수 있음
- => 문서나 데이터를 '크기와 방향'을 담는 벡터로 바꿔서 수치 구조로 바꿀 수 있음
- 내적
    - 두 벡터가 얼마나 같은 방향으로 향해 있는지를 반영한 값
    - 각 성분을 곱한 뒤 모두 더한 하나의 값
- '크기'와는 무관하게 '방향'의 유사도를 확인(정규화 진행)
- 내적으로 구한 값에 대해서 각 벡터의 크기로 나누기
    - 벡터의 크기: 벡터가 원점으로부터 떨어져 있는 거리
- 두 벡터가 얼마나 같은 방향으로 향해 있는지를 반영한 값(두 벡터의 내적('크기'))
- 크기와 무관하게 '방향'만을 확인하기 위해 정규화 (각 벡터의 크기로 나누기)
- '크기'가 아닌 '방향'만 확인하는 이유
    - 검색/추천 에서 중요한 건 '어떤 단어/특징이 얼마나 중요한가?'이지, '몇번 출현?'이 아니기 때문
    - 문서가 불필요하게 길어지거나 한 쪽의 출현 횟수만 늘려도 패턴만 확인해서 비교할 수 있기 때문에 순서와 비율을 정확히 비교 가능

## 문서 벡터화
- 두 벡터 간의 유사도를 알아내기 위해 벡터 값이 필요
    1. 주어진 단어를 모두 나열
        - 문서별 단어 등장 횟수를 저장(크기)
    2. 각 차원의 의미를 (임의로)정의 => 문서 벡터화 결과(v1,v2)
        - v1의 의미: 문장 안에 과일 단어의 비율
        - v2의 의미: 문장 안에 주관적 표현의 비율
        - 정의한 내용에 따라 벡터화
- 문서를 컴퓨터가 연산할 수 있는 수치 벡터로 바꾸는 과정
- 필요성
    - 컴퓨터는 문자열 자체를 수학적 연산에 바로 쓸 수 없음
    - 문서의 내용/구조/의미를 수치(벡터)로 옮겨야만, 검색/추천/분류 알고리즘을 적용할 수 있음
- 대표적인 벡터화
    - 카운트 기반
    - 임베딩 기반
        - Word2Vec
        - Doc2Vec
        - 생성형 AI 기반 임베딩

## 카운트 기반 벡터화
- 문서를 벡터로 바꿀 때 '단어가 얼마나 자주 나오는 지'을 이용하는 방법
- 말뭉치(corpus, 문서 모음)내 모든 단어로 사전을 만들고, 각 문서는 단어마다 단어 등장 횟수를 세어 벡터로 표현하는 방법
- 대표 기법
    - BoW(Bag of Words): 순서/문맥 무시, 등장 횟수만 기록
    - TF-IDF(TF-inverse Document Frquency): 흔한 단어 가중치 감소
        - 단어 등장 횟수, 희소성(가중치 조정기법)
    
## Bag of Words
- 고유 단어 추출
    - 불용어(조사, 접속사 등)를 제거 -> 더 높은 정확도
- 각 단어에 인덱스 부여
    - 사전순 혹은 등장 순서대로 인덱스 부여
- 문서 별 각 단어의 등장 빈도수 기록
    - 문서 내 등장 횟수를 해당 단어의 인덱스 위치에 기록
- 문서 별 빈도 벡터 정보
    - 단, 아래 예시는 편의상 불용어 제거 과정을 거치지 않음
    - 문서(단어) 벡터간 유의미한 유사도를 계산 불가
        - 단어의 출현 횟수를 수로 작성할 뿐, 단어의 의미 자체를 전혀 반영하지 않음
        - '자동차'와 '차'는 같은 의미이지만, 다른 단어로 취급
    - '사과'와 '바나나'는 과일이라는 공통점을 가지고 있으나, 해당 정보를 단어 카운트 행렬에서는 반영할 수 있는 방법이 없음
    - => 즉, 과일이라는 공통점을 연관시키지 못함
    - 어휘 크기만큼 벡터 길이가 커지고, 메모리/연산 부담이 큼

## 임베딩 기반 벡터화
- 워드 임베딩
    - 텍스트를 '의미'를 반영한 숫자 벡터로 변환하는 기법
    - 단순히 단어 등장 횟수를 세는 방식(Bag of Words)으로는 '사과'와 '배'가 과일이라는 의미적 유사도를 충분히 반영하기 어려움
    - 임베딩 과정을 통해, 서로 의미가 비슷한 단어,문장이 고차원 벡터 공간 상에서 가까이 위치하도록 학습 시킴
    - 임베딩 된 벡터들은 서로 코사인 유사도 등으로 간단히 비교 가능하며, 훨씬 정교한 문서(단어) 간 유사도 계산이 가능해짐

- 임베딩 기반 벡터화 순서
    1. 데이터 수집
    2. 데이터 전처리
        - 토큰화: 띄어쓰기 기준으로 분리
        - 한글이 아닌 표현 제거
        - 불용어 제거: 벡터로 만들 필요가 없거나 지나치게 많이 사용되는 단어 제거(조사 등)
    3. 임베딩용 딥러닝 모델 설계 (Word2Vec, Doc2Vec 등 사용 예정)
        - Word2Vec
            - 단어 관계를 학습해 각 단어를 의미 벡터로 만드는 방법
            - 분포 가설에 기반하여 단어의 의미를 벡터로 '학습'
                - 분포 가설: '서로 비슷한 맥락에서 등장하는 단어들은 의미도 비슷하다'
            - 주변 단어를 보고 중심 단어를 맞히거나 학습하면, 단어 간 의미 관계가 유사한 벡터로 학습
            - 학습 방식
                - CBOW : 중심 단어를 예측
                - Skip-gram : 주변 단어들을 예측
            - 활용
                - 불용어 처리 시 상황에 따라 적절한 처리가 필요한 이유
                    - 언어별 특성을 잘 반영하여야 함
                    - 문서/도메인의 특성이 잘 반영되어야 함
            - 정리
                - 분포 가설에 기반하여 단어의 의미를 벡터로 학습
                    - 분포 가설: 서로 비슷한 맥락에서 등장하는 단어들은 의미도 비슷하다.
                - '단어 벡터 공간'에서 유사 단어끼리는 가깝게, 다른 단어는 멀게 배치해서 유사도 측정
                - 학습/추론 속도가 빠르고, 의미적 유사도를 잘 포착하지만
                - 문맥별 의미 파악이 불가능
                    - 탈 것 '차'와 마시는 '차'를 구분 못함
                - 학습 당시 사용한 말뭉치에 없는 단어는 벡터를 얻을 수 없음
                - 문서 전체 맥락 이해하기엔 부족
                
        - Doc2Vec
            - 문서와 단어 관계를 함께 학습해 문장/문단 전체를 하나의 벡터로 만드는 방법
            <h3>Document Embedding
            - 여러 단어로 이루어진 문장, 문서를 임베딩하는 방법
            - 문서별로 고유 문서 태그(문서 ID)를 부여
            - Word2Vec과 유사하게 중심 단어와 주변 단어를 예측
                - 문서(문장)를 대표하는 문서 태그를 은닉충에 함께 학습
                - 이 문서 태그 임베딩이 결국 해당 문서를 대표하는 벡터가 됨
            - 문서의 단어들을 입력으로 받고(또는 Skip-Gram이면 단어를 예측)
            - Doc2Vec 모델을 사용하여 '문서 태그 + 단어 임베딩'을 동시에 업데이트
            - Doc1: 해당 문서를 대표하는 벡터
                - 껴서 하면 그 문서의 가중치를 알 수 있음
            - 만족스럽지 않은 결과가 도출된 이유
                - 문장 길이가 너무 짧고 단어 수가 적음
                    - 학습 정보 부족, 모델이 제대로 구분할 만한 특징 잡기 어려움
                - 주제별 키워드가 불충분하거나 겹치는 어휘가 많음
                    - '기계학습', '데이터' 등 핵심 단어가 충분히 반복되어야 함
                - 학습 파라미터 및 데이터 규모 한계
                    - 소수의 짧은 문장만으로 Doc2Vec을 학습하면, 토픽별 임베딩이 명확히 분리되지 않을 수 있음
            - 정리
                - 단어 수준(Word2Vec) 에서 문단 수준으로 확장하여 단어의 의미를 벡터로 학습하는 기법
                - '문서 벡터 공간' 에서 유사 문서(문단)끼리는 가깝게, 다른 문서는 멀게 배치해서 유사도 측정
                - 문서 전체 의미를 직접 반영한 벡터를 획득할 수 있지만
                - 문서 수만큼 추가 파라미터가 필요 => 메모리/학습 비용 증가
                - 같은 문서 내의 다의어는 구분하지 못함
                    - 같은 문서 내의 탈 것'차', 마시는 '차' 구분 못함
                - 여전히 학습 당시 사용한 말뭉치에 없는 단어는 벡터를 얻을 수 없음
        - 생성형 AI 기반 임베딩
            - 대규모 언어 모델(LLM)에서 제공하는 임베딩을 활용



## 생성형 AI 기반 임베딩
- GPT 계열 같은 대규모 언어 모델(LLM)에서 제공하는 임베딩 기능 활용
    - 이미 엄청난 텍스트 코퍼스(말뭉치)로 사전 학습되어 있음
    - 복잡한 자연어 전처리 불피요(토큰화, 불용어 제거 등)
- 문장/문서를 입력하면, 모델 내부의 숨은 표현을 활용해 의미가 반영된 벡터를 얻을 수 있음
- API로 텍스트 전달 => 임베딩 벡터 반환
    - Open AI, upstage 등
    - 수업때는 text-embedding-3-small (가장 저렴)

## pickle
- 파이썬 객체 구조의 직렬화와 역 직렬화를 위한 바이너리 프로토콜을 구현(백체 데이터 직렬화)

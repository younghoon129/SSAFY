# Spark RDD
- RDD의 개념과 특징
- RDD 생성 및 변환

## RDD의 개념과 특징
- RDD 정의
    - 대용량 데이터를 분산 처리하고 분석하기 위한 Spark의 기본 데이터 처리 단위
    - Resilient (탄력적인)
    - Distributed (분산된)
    - Dataset (데이터셋)

- RDD 특징
    - 데이터의 추상화
    - 탄력성(Resilient), 불변성(Immutable)
        - RDD는 한 번 생성되면 변경할 수 없음 (수정이 아닌 새로 생성)
        - 어떤 노드(서버)가 장애로 인해 중단되더라도, 데이터 복구 가능

    - 타입의 안정성 보장
        - 어떠한 하나의 타입의 객체를 가질 수 있음(모든 요소가 동일한 자료형)
        - 데이터 타입을 컴파일 시점에 검사
        - 성능 최적화
        - 코드의 가독성과 유지보수성 향상
        - Pyspark를 쓸 때는 Python이 동적 타입 언어이기 때문에 타입 안정성이 적용되지는 않음

    - 정형, 비정형 데이터
        - 비정형 데이터
            - 고정된 포맷이 없는 텍스트 데이터
            - sc.textFile()을 이용해 RDD로 로딩 후 map, filter, flatMap 등으로 가공
        - 정형 데이터
            - 컬럼이 있는 테이블 형태 데이터
            - DataFrame 또는 RDD.map()으로 가공

    - 지연 평가
        - 중간 연산을 줄여 성능 최적화
        - 실행 계획을 최적화하여 성능 향상
        - 불필요한 연산 방지로 리소스 절약
    
## RDD 생성 및 변환
### RDD 생성
- 기존의 메모리 데이터를 RDD로 변환하는 방법
    - Python의 리스트(List)나 Scala의 컬렉션(Collection)을 RDD로 변환 가능
    - 이 방법은 주로 테스트나 작은 데이터 셋을 다룰 때 사용
- 외부파일(텍스트, CSV, JSON 등)에서 RDD를 생성하는 방법
    - 실무에서는 보통 파일이나 데이터베이스에서 데이터를 불러와야 함
    - sc.textFile('파일 경로'), spark.read.format('jdbc').option(...) 형태를 사용하여 외부 데이터를 RDD로 변환할 수 있음

- Parallelize()
    - 기존 메모리 데이터를 Spark의 RDD로 변환하는 역할
    - parallelize()는 메모리에 있는 데이터를 Spark 클러스터로 보낼 때 사용
        - 데이터가 클 경우 비효율적일 수 있어 소규모 데이터분석에 주로 이용
- sc.textFile()
    - 외부 파일에서 데이터를 직접 읽어와 RDD로 변환하는 역할
        - 일반적인 텍스트 파일 (CSV, 로그 파일 등), S3 -> 저장소에서 데이터 로드 HBase, Cassandra(C)* -> NoSQL 데이터베이스에서 읽기 등

### RDD 변환
- MAP(구조유지)
    - 전처리, 구조변경할 때 등 사용 가능(1:1 구조)
    - Shuffle(파티션 간 재분배) 발생 안함
    - RDD의 각 요소에 함수 f를 적용하여 새로운 RDD를 반환

- FLATMAP(펼쳐서 새로운 구조)
    - 한 아이템에서 쪼개져서 갯수가 늘어남(1:N 구조)
    - 전체적인 아이템 갯수 늘어날 수 있음
    - RDD의 모든 요소에 먼저 함수를 적용한 뒤, 그 결과를 평탄화(flatten)하여 새로운 RDD를 반환

- FILTER
    - 사용자 함수(조건)를 적용하여, 함수(조건)의 반환값이 true인 항목만 유지

- MAPPARTITIONS
    - RDD의 각 파티션에 함수 f 를 적용하여 새로운 RDD를 반환

- MAPPARTITIONS WITH INDEX
    - 원래 파티션의 인덱스를 추적하면서, RDD의 각 파티션에 함수를 적용하여 새로운 RDD를 반환

- KEYBY
    - 원래 RDD의 각 항목에 대해 하나의 쌍(pair)을 생성하여 Pair RDD를 만들고, 쌍의 key는 사용자 정의 함수에 의해 값으로 부터 계산
    - 셔플 발생하지 않음

- GROUPBY
    - 원래 RDD의 데이터를 그룹화하기 위해, 사용자 정의 함수의 출력값을 key로 하고 이 key에 해당하는 모든 항목들을 value로 가지는 쌍(pair)을 생성
    - 셔플 발생할 수 있다

- GROUPBYKEY
    - 원래 RDD에서 각 key에 해당하는 value들을 그룹화하고 그룹화된 value들을 모아, 원래의 key와 함께 새로운 쌍(pair)을 생성
    - 셔플 발생할 수 있다

- JOIN
    - 원래 RDD들에서 같은 키를 가진 모든 요소 쌍(pair)을 포함하는 새로운 RDD를 반환

- UNION
    - 두 개의 원래 RDD에서 모든 항목을 포함하는 새로운 RDD를 반환, 중복된 항목은 제거 X

- DISTINCT
    - 파티션 합치는 과정에서 shuffle 필요
    - 하나의 파티션에 모은 후 확인
    - 원래 RDD에서 중복을 제거한 고유한 항목들만 포함하는 새로운 RDD를 반환

- SAMPLE
    - 원래 RDD에서 통계적 샘플을 추출하여 구성한 새로운 RDD를 반환(복원추출 가능, 무작위 샘플링)
    - 셔플 없이 동작
    - Big data
        - 머신 수가 많아질수록 처리 효율이 높아짐
        - small data 추출을 통해 일부 샘플만으로도 통계적으로 의미 있는 근사치 확보
    
- 파티셔닝 재분배 (Repartition vs Coalesce)
    - Repartition
        - 파티션을 늘리거나 줄이거나
        - 단점: 전체 셔플링해야 됨, 비용, 성능 부담
    - Coalesce
        - 셔플링 없이 줄일 수 있어 파티션 줄일 땐 이거 사용할 것
        - 기존 파티션끼리 병합

- COALESCE
    - 파티션 수를 줄여서 구성한 새로운 RDD를 반환
    - 그냥 합쳐서 줄임

- PARTITIONBY
    - 사용자 정의 함수가 반환하는 파티션에 따라 원래 항목들을 배치하여, 지정한 개수의 파티션을 갖는 새로운 RDD를 반환

- Word Counting
    - 종류
        - GROUPBYKEY
            - 셔플 후 그룹핑된 값을 합침
            - 셔플 양 많을 수 있음 -> 느릴 수 있음
            - 네트워크 부하 많을 가능성
        - REDUCEBYKEY
            - 로컬에서 먼저 집계(부분합)
            - 셔플 발생하지만, 불필요한 데이터 이동 최소화해서 집계 완성 가능
    - 특징
        - 두 함수가 모두 사용 가능하다면, ReduceByKey를 사용
        - ReduceByKey는 셔플 전에 행을 결합하여 셔플해야 할 행의 수를 줄일 수 있음
            - 로컬 집계
            - 중간 결과의 크기를 줄일 수 있음

- ACTIONS
    - 변환된 RDD 데이터를 메모리로 가져오거나, 저장하거나, 집계하는 연산
    - 종류
        - collect()
            - 모든 데이터를 리스트로 반환
        - count()
            - 전체 요소를 하나로 결합
        - reduce()
            - 전체 요소를 하나로 결합
        - sum()
            - 요소의 합 반환
        - mean()
            - 평균 값 반환

- COLLECT()
    - RDD의 모든 항목을 하나의 리스트로 드라이버 프로그램으로 반환
        - 하나의 리스트로?

- COUNTBYKEY()
    - RDD에 있는 각 키의 등장 횟수를 세어 키와 그 개수로 이루어진 맵(map)을 반환

- REDUCE
    - 전체 RDD를 대상으로 축소(reduce) 연산을 수행
    - RDD의 모든 요소를 사용자 정의 함수를 이용해 요소와 중간 결과를 쌍(pairwise)으로 연속적으로 집계하여, 최종 결과를 드라이버 프로그램으로 반환

- SUM()
    - RDD에 있는 모든 항목의 합(sum)을 반환

- MEAN()
    - RDD에 있는 모든 항목의 평균(mean)을 반환
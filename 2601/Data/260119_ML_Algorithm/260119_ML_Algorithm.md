    #  데이터 분석을 위한 머신러닝 알고리즘1
- 선형변환 및 행렬 연산(벡터)
- Regression(회귀)
- 베이즈 정리, 나이브 베이즈
- Classification(분류)
- KNN(주변 이웃 기반 분류)

## 선형 변환 및 행렬 연산 이론
- 머신러닝과 데이터 분석에서 행렬의 중요성
    - 데이터를 효율적으로 표현하고, 모델을 학습시키기 위해 행렬 사용(데이터를 수학적으로 다룸, 숫자데이터를 효율적으로 처리하는 과정을 위해 행렬)
    - 데이터 표현: 대부분의 데이터(이미지, 텍스트, 수치 데이터)는 행렬 형태로 저장
    - 머신러닝 알고리즘: 선형 회귀, 로지스틱 회귀, 신경망 등에서 가중치와 데이터 처리 연산 및 데이터 변형, 차원 축소를 위해 행렬 사용
    - 최적화 및 학습 과정: 경사 하강법 등 최적화 알고리즘 및 딥러닝에서 미분과 행렬 연산 사용

## 행렬
- 행렬이란?
    - 숫자가 행과 열로 배열된 2차원 구조
- 행렬 덧셈과 스칼라 곱
    - 행렬 간 덧셈: 같은 크기의 행렬끼리 요소별로 더함
    - 스칼라 곱: 행렬의 각 요소에 숫자를 곱함
    - 행렬 곱:
        - 두 개의 행렬을 곱하는 곱하는 연산
        - A(m*n) * B(n*p) -> 결과 행렬 C(m*p)
        - 첫 번째 행렬(A)의 열 개수와 두 번째 행렬(B)의 행 개수가 같아야 곱셈 가능
        - 1*n 행렬과 n*1 행렬의 곱으로 표현하는 형태를 벡터의 내적이라 볼 수 있음
            - 유사도(같은 방향 확인)
        - 머신러닝에서 행렬 곱 활용
        - 선형회귀: 모델의 가중치와 입력 데이터(X)의 행렬 곱을 통해 예측값 계산
        - 신경망: 각 레이어에서 가중치와 입력 데이터의 행렬 곱 수행
- 전치 행렬
    - 행과 열을 바꾸는 연산
    - 행렬 A(m*n)의 전치 행렬 -> At(n*m)
    - 원소: At[i,j] = A[j,i]
- 선형 변환
    - 선형 변환은 한 벡터 공간의 벡터를 다른 벡터로 대응시키는 변환
    - 크기 조절, 회전, 대칭, 전단 같은 공간의 형태 변형을 표현
    - 수식으로는 행렬 곱 Ax 형태
    - 원점을 보존하는 것이 특징
    - 데이터의 구조를 바꾸거나, 복잡한 데이터를 다루기 쉽게 만드는 데 활용
- 평행 이동
    - 공간의 모든 점을 같은 방향, 같은 거리만큼 이동
    - 점들의 모양, 방향, 상대적 위치는 변하지 않음
    - 좌표에 일정한 값을 더하는 방식으로 구현
    - 원점도 함께 이동하므로 선형 변환은 아님
    - 데이터 분석에서는 전처리(센터링) 단계에서 사용
- 크기 변경(Scaling)
    - 벡터를 일정 배율로 확대 또는 축소하는 변환
    - 스칼라 곱으로 표현 가능

## 회귀(Regression)
- 회귀란?
    - 회귀의 정의
        - 연속적인 숫자 값을 예측하는 지도 학습의 한 유형
        - 입력 데이터를 기반으로 특정 목표 값(출력)을 예측하는 것이 목적
        - Training Data를 이용해서 데이터의 특성과 상관 관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자) 값으로 예측하는 것
    - 회귀 문제의 목표
        - 입력 변수(특징)를 사용하여 출력 값의 추세를 학습하고, 이를 기반으로 새로운 데이터에 대해 예측을 수행
    - 회귀의 활용 예시
        - 주택 가격 예측: 집의 크기, 위치, 연도 등의 정보를 바탕으로 가격을 예측
        - 주식 시장 분석: 과거 주가 데이터를 바탕으로 미래 주가를 예측
        - 날씨 예측: 기온, 습도, 풍속 등의 데이터를 사용해 내일의 기온을 예측
    - 회귀와 분류의 차이점
        - 회귀는 연속적인 값을 예측하는 문제(예: 온도, 가격)
        - 분류는 이산적인 범주를 예측하는 문제(예: 스팸/비스팸, 고양이/개)
    - 입력 변수(Features, 독립 변수)
        - 회귀 문제에서 입력 변수는 예측에 필요한 다양한 특성을 의미
        - 입력 변수는 하나일 수도 있고, 여러 개일 수도 있음
        - 예시
            - 주택 가격 예측에서의 입력 변수: 주택 크기, 위치, 방 개수 등
    - 출력 변수(Target, 종속 변수)
        - 회귀 문제의 출력은 예측하고자 하는 연속적인 값
        - 출력 변수는 모델이 학습하여 예측하는 목표 값
            - 주택 가격 예측에서의 출력 변수: 예측된 주택의 가격
    - 모델의 목적
        - 입력 변수(특성)와 출력 변수(목표 값) 간의 관계를 학습해, 새로운 입력이 주어졌을 때 적절한 출력 값을 예측
        - 예시: 주택의 크기와 위치 정보를 입력으로 받아 주택 가격을 예측

- 회귀의 종류
    - 단순 선형회귀: 한 개의 독립변수(x)로 종속변수를(y)를 예측. 선형적(1차항 이상의 연산없음). 직선으로 표현
    - 다중 선형회귀: 두 개 이상의 독립변수로 종속변수를 예측. 선형적. 3차원 이상에서 평면으로 표현
    - 다항 회귀: 2차항 이상의 항으로 게산. 다항식.
    - 로지스틱 회귀: 범주형 종속변수를 예측하는 분류 모델

- 선형회귀란?
    - 선형회귀 정의
        - 입력 변수(특성)와 출력 변수(목표 값) 간의 선형 관계를 가정하여 예측하는 모델
        - 데이터가 직선으로 표현될 수 있을 때, 가장 간단하고 직관적인 방법
    - 모델의 수식
        - y = wx(가중치) + b
        - y: 예측값, x: 입력값, w: 웨이트, b: 바이어스
    - 예측 방법
        - 주어진 데이터를 기반으로 최적의 직선(회귀선)을 찾아 입력 값에 대한 출력 값을 예측
        - 모델은 오차를 최소화하는 방향으로 직선의 기울기 w와 절편 b를 학습
    - 장단점
        - 장점
            - 해석이 용이, 계산 빠름
            - 비교적 단순한 문제에 대해 좋은 성능을 발휘
        - 단점
            - 입력 변수와 출력 변수 간의 관계가 비선형적일 경우 성능이 저하될 수 있음
            - 과적합의 위험이 있으며, 다중 공선성이 있을 때 문제가 발생할 수 있음
    - 사용예시
        - 주택 가격 예측: 주택 크기에 따른 가격 예측
        - 광고 비용과 판매량의 관계 분석

- 다중 회귀란?
    - 다중 회귀의 정의
        - 여러 개의 입력 변수(특성)를 사용하여 출력 변수를 예측하는 선형 회귀 모델
        - 단일 입력 변수를 사용하는 선형 회귀와 달리, 다중 회귀는 다수의 변수를 고려하여 보다 정확한 예측 가능
    - 모델의 수식
        - y= w1x1 + w2x2 + ... +wnxn + b
        - y: 예측값, x1,x2,...xn: 입력 변수들, w1,w2,...,wn: 각 특성의 가중치, b: 바이어스
    - 예측 방법
        - 각 입력 변수에 해당하는 가중치를 곱한 후, 그 값을 모두 더하여 예측 값을 계산
        - 모델은 모든 입력 변수의 영향을 고려하여 오차를 최소화하는 방향으로 가중치와 절편을 학습
    
- 다항 회귀란?
    - 다항 회귀의 정의
        - 입력 변수와 출력 변수 간의 비선형 관계를 모델링하는 회귀 방법
        - 선형 회귀와 달리, 입력 변수의 거듭제곱을 포함한 다항식을 사용하여 비선형 패턴을 학습
    - 장단점
        - 장점
            - 데이터가 비선형적인 경우, 선형 회귀보다 더 나은 예측 성능을 보임
            - 다양한 비선형 패턴을 유연하게 모델링할 수 있음
        - 단점
            - 과적합의 위험이 큼
            - 특히 다항 차수가 높아질수록 모델이 복잡해져 과적합 가능성이 증가
            - 데이터가 부족할 경우, 모델이 안정적이지 않을 수 있음
    - 사용 예시
        - 곡선 패턴이 있는 데이터 예측: 온도 변화에 따른 전력 사용량 예측
        - 주택 가격 예측: 방 개수와 가격 간의 비선형 관계 모델링
    - 모델의 수식
        - y = w1x^1 + w2x^2 + ... wnx^n + b
        - y: 예측 값, x: 입력 변수, wn: 각 특성의 가중치, b: 바이어스
    - 예측 방법
        - 입력 값 x의 거듭제곱을 사용해 곡선 형태의 관계를 모델링
        - 선형 회귀로는 설명할 수 없는 비선형 패턴을 더 잘 포착 가능
    
## 베이즈 정리, 나이브 베이즈
- 나이브 베이즈 종류
    - 각 특징들이 독립적(서로 영향을 미치지 않을 것)이라는 가정 설정
    - 베이즈 정리(Bayes Rule)를 활용한 확률 통계학적 분류 알고리즘
    - 베이즈 정리를 활용하여 입력값이 해당 클래스에 속할 확률을 계산하여 분류
    - 각 특징들이 독립이라면 다른 분류 방식에 비해 결과가 좋고, 학습 데이터도 적게 필요
    - 각 특징들이 독립이 아니라면 즉, 특징들이 서로 영향을 미치면 분류 결과 신뢰성 하락
    - 학습 데이터에 없는 범주의 데이터일 경우 정상적 예측 불가능
    - P(A|B): 사건 B가 발생했을 때, A도 같이 발생했을 확률

## Classification
- 분류란?
    - 분류의 정의
        - 분류는 입력 데이터가 여러 개의 카테고리 중 하나에 속하도록 지정하는 작업
        - 주로 지도 학습(Supervised Learning) 방식으로 이루어지며, 데이터의 특징(Feature)을 기반으로 해당 데이터가 어느 범주에 속하는지를 예측
        - Training Data를 이용해서 데이터의 특성과 상관 관계 등을 파악, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 어떤 종류의 값으로 분류 될 수 있는지 예측하는 것
    - 분류 문제의 목표
        - 학습 알고리즘은 함수 f 을 생성하여 입력 벡터 x가 어떤 카테고리 y에 속하는지를 예측
        - 예시: 이미지 인식에서 입력은 이미지의 픽셀 값으로 이루어지고, 출력은 이미지에 포함된 객체를 나타내는 카테고리 번호
    - 분류 문제의 예시
        - 객체 인식(Object Recognition): 이미지 속 사물을 인식하여 해당 사물이 무엇인지 분류하는 작업
        - 얼굴 인식(Face Recognition): 사진 속 인물을 인식하고 자동으로 태그하는 기술로, 사용자와 컴퓨터 간의 자연스러운 상호작용을 가능하게 함

- 분류와 회귀의 차이
    - 분류
        - 정의: 데이터를 미리 정의된 카테고리(범주)로 분류하는 작업
        - 출력값: 이산적인 값(카테고리 또는 레이블)
        - 예시: 이메일이 스팸인지 아닌지 구분, 암 진단 여부(양성/음성) 분류, 이미지 속 개체 인식
    - 회귀(Regression)
        - 정의: 연속적인 숫자 값을 예측하는 작업
        - 출력값: 연속적인 실수 값
        - 예시: 주택 가격 예측, 온도 변화 예측, 주식 시장 가격 예측
    - 핵심 차이점
        - 분류 -> 결과가 '카테고리'로 나타나며, 이산적인 값으로 분류
        - 회귀 -> 결과가 '숫자'로 나타나며, 연속적인 값을 예측

- 분류 문제의 입력과 출력
    - 입력 데이터 (Features, 입력 변수)
        - 분류 문제에서 입력 데이터는 하나 이상의 특징(Feature)으로 이루어진 벡터로 표현
        - 특징 벡터
            - 예시: 이미지 인식에서 이미지의 각 픽셀 값이 특징이 될 수 있음
            - 입력 데이터는 연속형(숫자) 또는 이산형(범주형) 값으로 구성될 수 있음
    - 출력 데이터(Lables, 출력 변수)
        - 분류 문제의 출력은 데이터가 속한 클래스(범주)를 나타냄
        - 출력 레이블: y
            - 예시: 이진 분류에서 출력은 0(음성) 또는 1(양성)
            - 다중 클래스 분류에서는 y가 1,2,3 등 여러 클래스를 나타냄
        - 입력과 출력의 관계
            - 분류 모델은 주어진 입력 x 에 대해 적절한 출력 y 를 예측하는 함수 f(x)를 학습
            - 예시: f(x)는 특정 사진을 입력받아 그것이 고양이인지, 개인지, 또는 새인지를 예측
- 결정 경계란(Decision Boundaries)?
    - 정의
        - 분류 모델이 데이터를 분류하기 위해 공간을 나누는 경계선
        - 입력 데이터를 특징 공간(Feature Space)에서 서로 다른 클래스 영역으로 구분
    - 역할
        - 각 클래스에 속하는 데이터 포인트를 구분하는 기준을 제공
        - 새로운 데이터가 주어졌을 때, 그 데이터가 어느 클래스에 속하는지 결정
    - 예시
        - 선형 결정 경계
            - 단순한 직선이나 평면으로 클래스 간의 경계를 나눔
            - 예: 로지스틱 회귀, 선형 서포트 벡터 머신(SVM)
        - 비선형 결정 경계
            - 더 복잡한 경계를 만들어 클래스 간의 경계를 나눔
            - 예: 결정 트리, 랜덤 포레스트, 심층 신경망(Deep Neural Networks)
    - 시각화
        - 2차원 특징 공간에서는 결정 경계를 쉽게 시각화 가능
        - 각 클래스 영역이 어떻게 나누어져 있는지를 시각적으로 표현하면 모델의 동작을 더 쉽게 이해할 수 있음

- 분류 문제에 회귀 알고리즘 적용
    - 일반적인 회귀 알고리즘은 분류 문제에 그대로 사용 불가(-무한, +무한의 값을 가질 수 있기 때문)
    - 로지스틱 회귀(Logistic Regression)
        - 해당 클래스에 속할 확률인 0 또는 1 사이의 값만 내보낼 수 있도록 선형 회귀 알고리즘 수정하기
        - 이처럼 분류 문제에 적용하기 위해 출력 값의 범위를 수정한 회귀를 로지스틱 회귀(Logistic Regression)라고 함
        - 최소값 0, 최대값 1로 결과값을 수렴시키기 위해 Sigmoid(Logistic) 함수 사용

- 로지스틱 회귀(Logistic Regression)란?
    - 정의
        - 이진 분류를 위한 대표적인 선형 모델(S자형 곡선을 가지는 함수)
        - 입력 데이터를 기반으로 특정 클래스에 속할 확률을 예측
        - 회귀라는 이름이 붙었지만 분류 문제에 사용
    - 로지스틱 함수(Sigmoid 함수)
        - 예측값을 0과 1 사이의 확률로 변환하는 함수
    - 장점
        - 해석이 간단하고 계산이 빠름
        - 데이터가 선형적으로 분리될 수 있는 경우 좋은 성능
    - 단점
        - 비선형 데이터에는 적합하지 않을 수 있음
        - 복잡한 결정 경계를 만들기 어려움

- 결정 트리 (Decision Tree)란?
    - 정의
        - 데이터를 feature에 따라 트리 구조로 분할하여 분류하거나 예측하는 알고리즘
        - 각 노드는 특성의 조건을 기반으로 데이터를 분기하며, 최종 리프 노드는 예측 결과(클래스)를 나타냄
    - 작동 방식
        - 특성 선택: 각 특성을 기준으로 데이터를 분할할 수 있는 최적의 조건을 찾음
        - 분할: 데이터를 해당 특성 조건에 따라 좌우로 분기함
        - 리프 노드 도달 시 각 분할된 노드가 특정 클래스로 분류될 때까지 분할을 반복함
    - 특성 선택 기준
        - 지니 지수(Gini Index, 분류가 잘못될 확률) 또는 엔트로피(Entropy -> 5:5 일 경우 엔트로피가 크다 라고 정의)를 사용하여 분할의 순도를 측정
        - 순도가 높을수록(한쪽 클래스로 치우칠수록) 해당되는 특성은 더 좋은 분할 기준(값이 작으면 좋음)
    - 장단점
        - 장점
            - 해석이 용이: 트리 구조로 시각화가 가능하며, 결과 해석이 직관적
            - 비선형 데이터에도 적합: 복잡한 결정 경계를 만들 수 있음
            - 데이터 전처리 과정이 간단하며, 범주형 데이터도 다룰 수 있음
        - 단점
            - 트리가 너무 깊어지면 과적합의 위험이 있음
            - 작은 변화에도 모델이 민감하게 반응할 수 있음
            - 데이터가 많아질수록 성능 저하가 발생할 수 있음
        - 사용예시
            - 고객 이탈 예측, 의사결정 지원 시스템, 의료 진단 등 다양한 분야에서 사용

## KNN
- 대표적인 지도학습 알고리즘 중 하나
- k- 최근접 이웃 알고리즘 (k-Nearest Neighbors, k-NN) 이란?
    - 지도 학습에서 사용되는 간단한 분류 알고리즘
    - 데이터 포인트가 주어졌을 때, 그 데이터와 가장 가까운 k개의 이웃을 기준으로 클래스를 결정
    - 유사한 특성을 가진 데이터는 유사 범주에 속하는 경향이 있다는 가정 하에 분류
- 하이퍼파라미터 k
    - k 값은 미리 정해지는 값이며, k가 너무 작으면 과적합 발생, 너무 크면 과소적합 발생 가능성 있음
- 작동 방식
    - 거리 계산: 새로운 데이터와 훈련 데이터 간의 거리를 계산
        - 참고용
        - 위클리디안거리(L2 거리, 대각선으로 표현)
        - 맨하탄거리(L1거리, 격자(X, Y)로 표현)
    - k개의 이웃 선택: 가장 가까운 k개의 이웃을 선택
    - 다수결 투표: 선택된 이웃 중 가장 많은 클래스에 속하는 클래스를 새로운 데이터의 클래스로 예측
- 장단점
    - 장점
        - 직관적이고 이해하기 쉬움
        - 복잡한 결정 경계도 학습할 수 있음
    - 단점
        - 데이터가 많아지면 계산 비용이 많이듦
        - 차원의 저주(Curse of Dimensionality)에 취약함
        - 중요한 특징을 자동으로 학습하지 않으므로 Feature Engineering이 필요할 수 있음
    - 사용 예시
        - 이미지 인식, 추천 시스템, 패턴 인식 등에서 사용

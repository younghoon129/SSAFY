# Pine-tuning
## RAG
- 정의
    - 외부 지식 베이스(Knowledge Base)에서 정보를 검색(Retrieval)하고, 검색된 정보를 바탕으로 LLM이 답변을 생성하는 하이브리드 접근 방식
- 핵심 비유
    - 똑똑한 학생(LLM)에게 오픈북(외부 지식)을 쥐여주고 시험을 보게 하는 것
- RAG 나이브 도식도
    - Simple RAG


## ROPE
- 기존의 PE
    - 각 위치에 대해 고정된 벡터를 사용
    - 모델 학습 과정에서 각 위치 벡터를 학습
    - 단어의 순서 정보를 모델에 제공
    - 고정된 위치 벡터를 사용하여 절대적 위치 정보를 제공
    - 정적이며 문서 길이에 따른 일반화가 어려움
    - 예시 1에서 quick이 위치 2에 있으므로, 벡터 0.3, 0.4 반면 예시2는 위치 5에 있으므로, 벡터 0.9, 1.0 가짐
    - quick이라는 단어가 두 문장에서 다른 벡터를 가지게 되어 단어 간의 관계를 이해하는 데 어려움
- ROPE
    - 각 단어의 위치벡터에 회전 변환 적용, 상대적 위치 정보를 제공
    - 회전 변환을 통해, 단어 간의 상대적 위치를 잘 이해할 수 있게
    - 어텐션 메커니즘이 입력 벡터의 위치를 더 잘 이해, 성능 향상
    - 예시 1에서 quick이 위치 2, 벡터 0.3*cos(02), 0.4*sin(40_5)
    - 0(세터) 값은 단어간의 거리에 거리를 반영하여 계산됨. 그 위치와 주변 단어들과의 상대성 반영
    - 회전 변환을 적용하여 상대적 위치 정보를 제공하여 단어가 다른 단어들과의 상대적 위치를 통해 문맥을 더 잘 이해하게


## RLHF
- RLHF의 필요성
- human feedback 에서 소개된 기법
- 환각(Hallucination)
- 지시 불이행
    - Instcut tune을 진행하여도, 사용자의 Instruction을 따르지 않고, 멋대로 Context를 생성하며, user의 instruction에 대한 이해가 부족
- 편향 및 toxic
    - 편향이 있거나 악성 데이터로 훈련이 된 모델은 텍스트를 생성한 문장 자체에 편향과 악성 내용이 담겨져 출력을 하게 되는 문제점이 존재
- Token이 이산적이다
    - 수학적인 문제
- 데이터의 문제


## RLHF 방법론
- SFT Training
- Reward Model
- PPO
    - PPO는 강화학습에서 정책을 최적화하기 위한 알고리즘 중 하나
- 문제점
    - 너무나도 긴 학습 Framework
    - 이로 인해 발생되는 Cost

- DPO의 아이디어의 적용
- DPO는 기존의 접근 방식과 비슷하지만 조금 다르게 선호도를 직접 적용하여 policy를 최적화 하는 간단한 접근 방식을 적용
- Reward 모델 대신에, preference 데이터를 활용하여 최적화
- 핵심적인 아이디어 적용은, reward function과 policy 사이의 수치적 관계를 이용
- 복잡한 보상 모델을 만들지 않고도 선호도 데이터를 통해 직접 policy를 최적화
# 오전 실습
- 스테이블 디퓨전 이미지 생성 구현
- CLIP 모델 사용 생성 이미지 평가
- ResNet50 모델 결과 비교
- 생성된 데이터로 ResNet18모델 전이학습




## 스테이블 디퓨전 이미지 생성 구현
- 확산형 이미지 생성모델(Stable Diffusion) - 약간 나노바나나 기반 느낌인 거 같음
- 텍스트를 이미지로 만들어주는 마법
- 잠재공간 활용
  - 압축된 저차원 잠재 공간에서 노이즈를 추가하고 제거하여 계산 효율성 극대화


## CLIP 모델 사용 생성 이미지 평가
- 대조적 자연어-이미지 사전학습
- 언어와 이미지를 '대조'하면서 함께 학습한 모델
- 대조 목표를 사용하는 이미지 이해용 신경망 모델과 텍스트 이해용 신경망 모델을 쌍으로 훈련
- 즉, 하나의 이미지와 그 이미지를 설명하는 텍스트를 매칭시키는 모델
- 트랜스포머 구조(인코더 + 디코더) 중 인코더구조만 사용
  - 모델의 목적이 이해, 표현 이기 때문

## ResNet50 모델 결과 비교
- 어떤 이미지를 줘도 학습한 라벨링에서만 정답으로 대답 가능
  - 라벨링 과일만 있을 때 자동차 주면 과일 대답
## 생성된 데이터로 ResNet18모델 전이학습



# 오후 이론

## 텍스트 파운데이션 모델
- 텍스트 파운데이션 모델(거대 언어 모델)의 특이점
  - 규모의 법칙: 더 많은 데이터, 큰 모델, 긴 학습 -> 더 좋은 성능
  - 창발성: 특정 규모를 넘어서면 갑자기 모델에서 발현되는 성질
    - ex) 인-컨텍스트 학습: 주어진 설명과 예시만으로 새로운 데스크를 수월하게 수행
    - ex) 추론 능력
- 폐쇄형 거대 언어 모델
  - 장점:
    - 일반적으로 더 우수한 성능 및 최신 기능을 갖고 있으며 사용하기 쉬움
  - 단점:
    - 사용시 마다 비용 발생, 모델이나 출력에 대한 정보가 제한적으로 제공
  - 종류
    - ChatGPT: 가장 많은 활성 유저 수. 전반적으로 뛰어난 성능
    - Claude: 안전 지향적 모델. 코딩 관련 작업에 특히 뛰어난 성능
    - Gemini: 가장 긴 입력 및 출력을 지원. 뛰어난 멀티 모달 성능
- 개방형 거대언어 모델
  - LLaMA(Meta), Gemma, Qwen
  - 장점: 무료로 다운로드 및 사용 가능, 모든 정보가 공개되어 있음
  - 단점: 충반한 계산 자원 필요, 상대적으로 폐쇄형 모델에 비해 성능이 낮은 편


## 지시 학습
- 학습 테스크의 개수
  - 다양한 종류의 지시를 학습할 수록 보지 못한 지시에 대한 일반화 성능이 좋아짐
- 추가 학습하는 모델의 크기
  - 특정 규모 이하에서는 지시 학습의 효과성이 떨어짐 -> 지시를 이해하고 응답하는 것도 창발성의 하나!
- 지시를 주는 방법: 자연어 지시로 사람에게 대화하듯 지시하는 것이 가장 효과적
- 한계
  - 주어진 입력에 대해 적절한 하나의 응답이 있다고 가정
  - 정답이 정해져 있는 객관적 테스크에서는 자연스럽지만
  - 정답이 정해져 있지 않은 개방형 테스크에서는 한계가 있음


## 선호 학습(다양한 응답 중 사람이 더 선호하는 응답을 생성하도록 추가학습)
- PPO, DPO(Direct Preference Optimization, 직접 선호 최적화) 알고리즘 방식
- Deepseek-R1(아하 모뭔?)
  - 스스로 학습하다가 뭔가 잘못됨을 순간 깨달음
- 다양한 응답은 모델이 생성, 응답간의 선호도는 사람이 제공
- instructGPT의 핵심 아이디어
  - RLHF(사람의 피드백)을 통한 강화학습
- instructGPT 학습방법
  1. 지시 학습을 통한 텍스트 파운데이션모델의 추가 학습
    - 실제 유저로부터 다양한 지시 입력을 수집, 해당 입력에 대해 훈련된 사람 주석자들이 정답 데이터를 생성
  2. 사람의 선호 데이터를 수집, 보상 모델을 학습
    - 주어진 입력에 대한 선택지는 모델이 생성, 다양한 선택지에 대한 선호도는 사람이 생성
    - 사람과 일치한 선호도를 출력할 수 있도록 보상 모델을 지도 학습
  3. 보상이 높은 응답을 생성하도록 강화 학습을 통해 추가 학습
    - 1, 2 에서 보지 못한 질문에 대해 사람의 추가적인 개입 없이 학습된 모델들을 통해 추가 학습 진행
    - 지시 학습된 모델을 보상 모델 기반 강화 학습을 통해 한번 더 추가 학습
  - 결과
    - 기존 대비, instructGPT는 해로운 응답 덜 생성
    - 거짓말은 늘어날 수 있음 인간의 피드백으로 다양한 답변을 하다 보니
- LLaMA2(가장 많이 사용하는 모델 중 하나)
  - instructGPT와 비슷하게 RLHF와 대화 데이터를 활용
  - 당시 대화형 타입 개방형 거대 언어 모델 중 가장 우수한 성능


## 거대 언어 모델의 추론
- 디코딩 알고리즘
- 거대 언어 모델의 자동회귀 생성(Auto-regressive Generation)
  - 학습 완료도니 거대 언어 모델은 순차적 추론을 통한 '토큰별 생성'
  - Goal
    - 주어진 입력에 대해 다음 토큰을 생성(확률 분포를 제공)
  - 디코딩 알고리즘
    - 입력으로부터 다음 토큰을 생성하는 알고리즘 (다음 단어를 선택하는 방법)
    - generation()
    - 종류
      - 그리디: 확률이 가장 높은 다음 토큰을 선택
        - 장점
          - 사용하기 쉬움
        - 단점
          - 직후만 고려하기 때문에 생성 응답이 최종적으로 최선이 아닐 수 있다.
      - Beam Search: 확률이 높은 k개(beam size)의 후보를 동시에 고려
        - 고르는 기준: 누적 생성확률(지금까지 생성한 문장 전체가 나올 확률의 곱)
        - 전ㄹ체 문장 후보들의 누적 확률을 기준으로 상위 k개를 남기는 것
        - 장점
          - 최종적으로 좋은 응답 생성 확률 높음
        - 단점
          - 계산 비용 많이 늘어남(각 후보마다 LLM 추론을 수행하기 때문)
      - Sampling: 거대 언어 모델이 제공한 확률을 기준으로 랜덤하게 생성
        - 장점
          - 다양한 응답을 생성할 수 있음
        - 단점
          - 생성된 응답의 품질이 감소할 수 있음
      - Sampling 'with Temperature'
        - 하이퍼 파라미터 T를 통해 거대 언어 모델이 생성한 확률 분포를 임의로 조작
        - 기존에 확률이 높은 응답에 집중
      - Top-K Sampling
        - 확률이 높은 K개의 토큰들 중에서만 랜덤하게 확률에 따라 샘플링
        - 문맥에 따라 다음 단어의 예측 확률 합이 다르다
        - 장점
          - 품질이 낮은 응답을 생성할 가능성을 줄일 수 있음
        - 단점
          - 확률 분포의 모양에 상관 없이 고정된 K개의 후보군을 고려
      - Top-P Sampling (or Nucleus Sampling)
        - K를 고정하는 대신, 누적 확률(P)에 집중하여 K를 자동으로 조절
        - ex) P=0.9 -> 확률이 높은 K개의 응답 후보의 확률을 더했을 때 0.9를 처음으로 초과하는 K를 사용
        - 다양한 평가 지표에서 기존 디코딩 알고리즘들 대비 좋은 성능을 달성 

## 프롬프트 엔지니어릴
- 원하는 답을 얻기 위해 모델에 주어지는 입력을 설계 조정하는 기법
- 입력 프롬프트 = (1)지시 + (2) 예시
  - 어떻게 지시를 주는지, 어떤 예시를 보여주는지가 거대 언어 모델의 성능에 크게 영향을 미침
- 지시
  - 감정 분류 뿐만 아니라 수학, 코딩과 같은 어려운 문제를 거대 언어 모델로 푸는 것에 많은 관심 집중
- Chain-of-Thought(CoT) 프롬프팅
  - step by step
  - 단순히 질문과 응답만을 예시로 활용하는 것이 아니라, 추론 과정도 예시에 포함
    - 이를 통해 테스트 질문에 대해 추론을 생성하고 응답하도록 유도, 더 정확한 정답 생성을 기대가능
    - 질문에 대한 정답 바로 제시 x
    - 질문에 대한 정답이 나오는 추론 과정을 함께 제시(정답률 상승)
  - 결과
    - CoT는 거대 언어 모델(PaLM)의 추론 성능을 크게 증가시킴
  - CoT로 인한 성능 향상은 모델 크기가 커질 수록 더 확대(추론~=창발성?)
    - 창발성: 모델 크기가 커지면 갑자기 새로운 능력이 나타나는 현상을 의미
  - "Let's think step by step"
    - 문구 추가로 예시 없이 LLM 성능 향상
- 0-shot CoT 프롬프팅
  - 유인 문장을 통한 추론 생성 ("Let's think step by step")
  - 주어진 질문과 생성된 추론을 통한 정답 생성 (Therefore, the answer is)
  - 특징 
    - 단순한 문구 하나가 성능을 크게 향상시킴
    - 반대로 적절하지 못한 문구를 제시했을 때 성능을 떨어뜨리거나 역효과
  - 결과
    - 0-shot CoT는 기존 0-shot 프롬프팅보다 훨씬 높은 추론 성능을 달성
    - 0-shot CoT는 모델 크기가 임계점을 넘어서야 효과성이 발휘
      - 따라서, 추론 능력은 거대 언어 모델의 창발성 결과로 볼 수 있음
- 수학쪽은 이미 너무 많은 정보가 있기 때문에 어설픈 예시를 제공하는 것보다 그냥 물어보는 게 더 좋은 답변 나올 수 있음

## 거대 언어 모델의 평가와 응용
- 변인통제
- 평가란
  - 구축한 시스템이 실제로 잘 동작하는지를 확인하는 단계
  - 평가의 3가지 요소
    - 목표: 시스템으로 무엇을 달성하고자 하는지
    - 평가 방법: 어떤 방법으로 평가할 것인지
    - 평가 지표: 어떻게 성공 여부를 판단할 지
- AI 모델의 평가: "테스트 데이터"
  - 핵심 가정: 학습 단계에서 본 적이 없고, 질문과 정답을 알고 있음
- 거대 언어 모델 평가의 특징
  - 특정 테스크에서 학습된 기존 AI 모델들과 달리, 거대 언어 모델은 다양한 테스크에 대해 동시에 학습됨
    - 따라서, 거대 언어 모델의 성능을 올바르게 평가하기 위해서는 많은 테스크에서의 성능을 종합적으로 판단해야함
    - 또한, 디코딩 알고리즘, 입력 프롬프트에 따라 같은 질문에 대해서도 예측이 바뀌므로, 공평한 비교를 위해서는 해당 부분도 고려해야함
- 거대 언어 모델 평가 방법의 종류
  - 정답 정해진 경우
    1. 예측과 정답을 비교하여 일치도를 측정(정확도)
    - MMLU 벤치마크: 57개의 다양한 전문 분야에 대한 객관식 문제들로 구성
    
  - 정답 안정해진 경우
    1. **사람이 임의의 정답**을 작성 및 이와 예측을 비교
        - 텍스트 간의 유사도를 측정
          - 단어 수준에서의 유사도 측정(ROUGE 등)
          - 벡터 공간에서의 유사도 측정(코사인 유사도) (cosine similarity embedding space)
    2. 정답과 무관하게 생성 텍스트 자체의 품질만을 측정
        - Perplexity(PPL): 얼마나 문장이 확률적으로 자연스러운지 측정
          - 다음 단어가 올 확률을 계산하기 위해 언어모델을 주로 활용
    3. 생성된 텍스트의 '상대적 선호'를 평가
        - 창의성, 유창성, 가독성 등: 각각에 부합하는 평가지표를 따로 설계하는 것은 굉장히 어려움
        - LMArena
          - **실제 유저 피드백 활용, 거대 언어 모델 성능 측정 방법 중 가장 신뢰성 있는 방법 중 하나로 여겨짐**
          - 단점: 높은 평가 비용 및 시간 필요
            - 이를 해결하기 위해 거대 언어 모델을 활용한 평가 나옴
        - 기존 방법: 전문가를 고용하여 평가를 맡김
  - 거대 언어 모델을 활용한 평가(심사위원 역할을 맡아 품질 평가 요청)
    - LLM-as-judge(or G-Eval): 거대 언어 모델을 통해 생성 텍스트를 평가
      - 유저는 (1) 풀고자 하는 테스크(질문) (2) 평가하고자 하는 텍스트 (3) 평가 기준을 제공
      - 거대언어모델은 평가 결과(점수, 이유)를 제공
    - 단점
      - 위치편향: 특정 위치의 응답을 상대적으로 선호(첫번째 응답 > 두번째 응답)
        - 순서 바꿔서 두 번 평가하고 평균을 취하는 것으로 해결
      - 길이편향: 품질과 무관하게 길이가 긴 응답을 상대적으로 선호
        - 길이가 미치는 영향을 통계적으로 제거해서 어느정도 해결할 수 있음
      - 자가선호편향: 생성 모델이 평가 모델과 같은 경우, 이를 선호
    - GPT-4를 활용한 평가는 기존 평가 지표(ROUGE)보다 더 사람과 유사한 결과를 보임
    - LLaMA3 학습을 위한 데이터 필터링에도 사용되는 등 다양한 어플리케이션에서 이미 활용되고 있음
    
## 거대 언어 모델의 응용
- 멀티모달 파운데이션 모델
  - ex) 멀티 모달 입력(이미지, 비디오, 오디오), 멀티모달(오디오, 텍스트) 출력 생성
  - 다른 모달리티 데이터를 거대 언어 모델이 이해할 수 있도록 토큰화 및 추가 학습
- 합성 데이터 생성
  - self-instruct: 175개의 데이터를 사람이 작성한 뒤, GPT-3을 통해 52000개의 합성 데이터 생성
  - 결과: 기존 instructGPT에서 활용된 사람이 만든 데이터와 비슷한 성능 달설
    - GPT+합성 데이터(39.9) : 사람 데이터 기반(40.8)
    - SuperNI 함께 학습시: 합성 데이터(51.6) > 사람 데이터 기반(49.5)
  - self-instruct
    - 사람이 만든 소량의 데이터를 기반으로 대규모 합성 데이터셋을 확장
    - 합성 데이터로 학습한 모델이 사람 데이터 기반 성능과 유사한 결과 달성
  - 필터링
    - 합성 데이터에서 중복,유사(유사도 높은) 데이터 제거
    - 모델이 더 폭넓은 지시문 학습이 가능하도록 다양한 instruction 확보
    - 결과: 중복이거나 무관한 합성 데이터는 제거, 새로운 지시문만 남겨 다양성 확보
  - Alpagasus: 프롬프팅을 통한 합성 데이터의 품질 평가 및 필터링 제안
    - Alpagasus
      - 저품질 합성 데이터를 걸러내고 고품질만 학습해, 빠르고 강력한 성능 달성 목표
    - 52000개 Alpaca 합성 데이터 중 9000개 정도만 4.5 이상의 점수 받음 -> 해당 데이터만 학습에 사용
    - 결과: 전체 합성 데이터를 사용한 경우보다 높은 성능을 달성
- 거대 언어 모델의 한계
  - 환각
    - 사실과 다르거나, 전적으로 지어낸 내용임에도 불구하고 정확한 정보와 동일한 자신감과 유창함으로 응답을 생성
      - 이는 거대 언어 모델이 확률적으로 다음 토큰 예측을 통해 응답을 생성하기 때문에 발생
      - 따라서, 사용자 입장에서 거대 언어 모델의 응답에 대한 진위성을 구별하기 어렵게 만듦
    - 사전 학습 데이터의 제한적인 범위가 환각 현상의 원인이 되기도 함
  - 탈옥
    - 프롬프팅 엔지니어링을 통해 거대 언어 모델의 정렬을 우회할 수 있다는 것이 확인됨
    - 여러 단계의 학습 과정에서 기인한 근본적인 한계 때문에 발생했으며, 다양한 탈옥/방어 방법이 활발히 탐구 중
  - AI 텍스트 검출
    - 거대 언어 모델의 무분별한 사용이 학교 및 회사에서 여러가지 새로운 문제를 만들고 있음
      - 거대 언어 모델이 만든 텍스트를 구분 또는 탐지 어느정도 가능
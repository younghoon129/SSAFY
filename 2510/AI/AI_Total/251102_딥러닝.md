# ✍️ 단답형 문제 (10문제)
문제 1. '가중치(Weight)'에 페널티를 주어 불필요한 가중치를 '0'으로 만드는 규제 기법은 무엇인가?

문제 2. '뉴런(Neuron)'을 학습 시 랜덤하게 '일시적으로 꺼서' 특정 뉴런에 대한 의존을 막는 규제 기법은 무엇인가?

문제 3. MLP 학습 4단계 중, 2단계(손실 계산)에서 계산된 오차를 '거꾸로 전파'하며 각 가중치의 '기울기(Gradient)'를 '계산'하는 단계는 무엇인가?

문제 4. 3단계(역전파)에서 계산된 '기울기'와 '학습률'을 받아, '가중치'를 '실제로 수정(업데이트)'하는 알고리즘은 무엇인가? (예: Adam)

문제 5. '분류' 문제에서 '오차'를 계산하기 위해 사용되는 대표적인 '손실 함수(Loss Function)'는 무엇인가? (힌트: 교차...)

문제 6. '회귀' 문제에서 '오차'를 계산하기 위해 사용되는 대표적인 '손실 함수(Loss Function)'는 무엇인가? (힌트: 평균...)

문제 7. '관성'과 '적응적 학습률'을 사용하여 가장 널리 쓰이는 '옵티마이저(Optimizer)'의 이름은 무엇인가?

문제 8. $max(0, x)$로 계산되며 '기울기 소실' 문제를 해결한 '활성화 함수(Activation Function)'의 이름은 무엇인가?

문제 9. 과적합은 모델의 복잡도가 너무 (높아서/낮아서) 발생하며, 규제는 이 복잡도에 페널티를 주어 모델을 더 (단순하게/복잡하게) 만든다. (괄호 안에서 선택)

문제 10. '활성화 함수(예: ReLU)'는 MLP 학습 4단계(순전파/손실/역전파/업데이트) 중 어느 단계에서 '계산'되어 사용되는가?

# 📜 서술형 문제 (5문제)
문제 11. 'L1 규제(Lasso)'와 '드롭아웃(Dropout)'은 모두 과적합을 막는 규제 기법입니다. 두 기법이 과적합을 막는 (1) 대상과 (2) 방식이 어떻게 다른지 비교 설명하시오.

문제 12. 딥러닝 학습 과정에서 '역전파(Backpropagation)'와 '옵티마이저(Optimizer)'의 역할을 명확히 구분하여 설명하시오. (즉, 역전파가 '무엇을' 계산하고, 옵티마이저는 그 '무엇을' 받아서 '어떻게' 하는지)

문제 13. '손실 함수(예: 교차 엔트로피)'와 '평가 지표(예: 정확도)'의 **역할(용도)**이 어떻게 다른지 설명하고, 왜 '정확도'를 '손실 함수'로 사용하기 어려운지 서술하시오.

문제 14. '과적합(Overfitting)'이 무엇인지, 그리고 '규제(Regularization)'가 과적합을 막는 '기본 원리'가 무엇인지 설명하시오.

문제 15. '기울기 소실(Vanishing Gradient)' 문제가 무엇인지, 그리고 'RELU' 활성화 함수가 이 문제를 어떻게 해결하는지(혹은 완화하는지) 설명하시오.

# 📝 단답형 / 서술형 예시 답안
단답형 정답

L1 규제 (Lasso)

드롭아웃 (Dropout)

역전파 (Backpropagation) (Q19, Q25 오답 관련)

옵티마이저 (Optimizer) (Q19, Q25 오답 관련)

교차 엔트로피 (Cross-Entropy) (Q22, Q68 오답 관련)

평균 제곱 오차 (MSE)

Adam (아담)

RELU (렐루)

높아서 / 단순하게 (Q71 오답 관련)

순전파 (Forward Propagation) (Q70 오답 관련)

서술형 정답 (예시) 11. (Q13 오답 관련)

(1) 대상: L1 규제는 '가중치(Weight)'를 대상으로 합니다. 드롭아웃은 '뉴런(Neuron)'을 대상으로 합니다.

(2) 방식: L1 규제는 불필요한 '가중치'를 0으로 만들어 해당 특성을 무시하게(특성 선택) 합니다. 드롭아웃은 학습 시마다 '뉴런'을 랜덤하게 꺼서, 모델이 특정 뉴런에 과도하게 의존하는 것을 방지하고 여러 모델을 학습하는 듯한 앙상블 효과를 줍니다.

(Q19, Q25, Q31 오답 관련)

역전파(Backpropagation): 2단계(손실 함수)에서 계산된 '오차(손실)'를 '거꾸로 전파'시키며, 이 오차에 각 '가중치'가 얼마나 기여했는지를 수학적 '미분(체인룰)'을 통해 '계산'하여 '기울기(Gradient)'를 구하는 역할까지입니다.

옵티마이저(Optimizer): 3단계(역전파)에서 계산된 '기울기'를 입력으로 받습니다. 그리고 이 기울기와 '학습률'(및 Adam의 경우 '관성' 등)을 조합하여, **'실제로 가중치를 수정(업데이트)'**하는 역할을 수행합니다.

(Q22, Q68 오답 관련)

역할(용도): **손실 함수(예: 교차 엔트로피)**는 '모델을 최적화(학습)'시키기 위한 '목표 함수'입니다. 즉, '미분 가능'해야 하며 '오차'를 정량화하여 역전파에 전달합니다. 반면, **평가 지표(예: 정확도)**는 학습이 잘 되었는지 '사람이 이해'하기 쉽게 '평가'하기 위한 지표입니다.

이유: '정확도'는 미분 불가능(non-differentiable)하거나, 대부분의 구간에서 기울기가 0입니다. (예: 예측 확률 0.49 -> 0.51로 바뀌면 정확도는 0 -> 1로 뛰지만, 그 사이 0.49 -> 0.48로 바뀔 땐 기울기가 0임). '기울기'를 계산할 수 없으므로 '역전파'와 '경사 하강법'에 사용할 수 없어 손실 함수로 부적절합니다.

(Q71 오답 관련)

과적합: 모델이 '너무 복잡'해져서, 훈련 데이터의 '핵심 패턴'뿐만 아니라 '잡음(Noise)'까지 모두 암기해버린 현상입니다.

규제의 원리: 과적합은 모델이 복잡해서(예: 가중치가 너무 커서) 발생하므로, '규제'는 가중치가 너무 커지지 않도록 '제약(페널티)'을 부과합니다. 이로써 모델을 '더 단순하게' 만들어 잡음을 무시하고 핵심 패턴에 집중하도록 유도하여 과적합을 막습니다.

기울기 소실: 얕은 신경망에서 쓰이던 시그모이드 함수는 미분값이 최대 0.25입니다. 층이 깊어져 역전파 시 0.25보다 작은 값을 계속 곱하면, 기울기가 '0에 가깝게 사라져서(Vanishing)' 입력층 근처의 가중치가 '학습(업데이트)되지 않는' 문제입니다.

RELU의 해결: RELU는 입력이 0보다 크면 미분값이 항상 '1'입니다. 1을 아무리 곱해도 기울기가 0으로 사라지지 않으므로, 층이 깊어져도 '기울기 소실' 문제를 해결(완화)할 수 있습니다.
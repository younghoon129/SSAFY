## 단답형 문제 (5문제)
이어서 단답형 및 서술형 문제입니다. 스스로 답을 생각해 보신 후, 아래 정답과 비교해 보세요.

- 문제 11. 연속적인 숫자 값을 예측하는 지도 학습 방법을 무엇이라고 하는가?

- 문제 12. 정답지가 없는 데이터에서 비슷한 데이터끼리 묶는 비지도 학습 방법을 통칭하여 무엇이라고 하는가?

- 문제 13. 데이터의 차원이 너무 많아 발생할 수 있는 과적합, 성능 저하, 학습 시간 증가 등의 문제를 무엇이라고 하는가?

- 문제 14. K-means와 달리 K값을 미리 정하지 않고, 거리를 기준으로 가까운 데이터부터 순차적으로 묶어 나가는 군집화 방법은 무엇인가?

- 문제 15. 소프트맥스 회귀에서 다중 분류를 위해 사용되며, '정답 확률값에만 집중'하는 특징을 가진 손실 함수는 무엇인가?

## 서술형 문제 (5문제)
- 문제 16. 지도 학습과 비지도 학습의 가장 큰 차이점을 '정답지' 유무를 중심으로 설명하고, 각각의 학습이 사용되는 예시를 하나씩 드시오.

- 문제 17. 로지스틱 회귀가 이진 분류에 사용되는 과정을 '선형 회귀', '시그모이드 함수', '이진 교차 엔트로피' 키워드를 모두 사용하여 설명하시오.

- 문제 18. '차원의 저주'란 무엇이며, PCA와 같은 차원 축소가 이 문제를 어떻게 해결하는지 설명하시오.

- 문제 19. K-means 군집화와 계층적 군집화의 차이점을 K값 설정 및 군집 형성 방식을 중심으로 설명하시오.

- 문제 20. 강화 학습이 '상과 벌'을 통해 학습하는  과정을 간략히 설명하시오.

## 단답형/서술형 예시 답안
- 단답형 정답 11. 회귀 (또는 선형 회귀) 12. 군집화 (Clustering) 13. 차원의 저주 (Curse of Dimensionality) 14. 계층적 군집 (Hierarchical Clustering) 15. 범주형 교차 엔트로피 (Categorical Cross-Entropy)

- 서술형 정답 (예시) 16. 지도 학습은 '정답지(레이블)'가 있는 데이터를 사용합니다. 즉, 입력(X)에 대한 정답(Y)을 알고 있는 상태에서 X가 주어졌을 때 Y를 맞추도록 학습합니다. (예: 스팸 메일 분류, 주택 가격 예측) 반면, 비지도 학습은 '정답지'가 없는 데이터를 사용하며, 데이터 자체의 숨겨진 구조나 패턴(유사성)을 파악하는 것이 목적입니다. (예: 고객 그룹 분류, 데이터 차원 축소) 17. 로지스틱 회귀는 먼저 입력 특성(feature)들을 선형 회귀 모델처럼 조합하여 값을 계산합니다. 이 값을 시그모이드 함수에 통과시켜 0에서 1 사이의 확률 값으로 변환합니다. 이 확률 값이 0.5보다 크면 1, 작으면 0으로 분류하며, 학습 과정에서는 이진 교차 엔트로피 손실 함수를 사용하여 실제 정답과의 오차(틀린 답에 확신하면 강한 오차)를 최소화하는 방향으로 모델을 업데이트합니다. 18. 차원의 저주란 데이터의 차원(특성)이 너무 많아질 경우, 데이터 공간이 너무 넓어져 데이터 밀도가 희박해지고, 이로 인해 과적합이 발생하거나 모델 성능이 저하되며 학습 시간이 오래 걸리는 문제를 말합니다. 차원 축소(예: PCA)는 데이터의 분산(정보)을 최대한 보존하는 새로운 저차원 축을 찾아, 원본 데이터를 이 축에 투영시킴으로써 핵심 정보는 남기면서 데이터의 복잡성(차원)을 줄여 이러한 문제를 해결합니다. 19. K-means는 사용자가 미리 군집의 개수(K)를 정해야 하며, K개의 중심점을 기준으로 각 데이터가 가장 가까운 중심점에 속하도록 그룹을 형성합니다. 반면 계층적 군집은 K값을 미리 정할 필요가 없으며, 개별 데이터에서 시작하여 거리가 가까운 데이터(또는 군집)끼리 순차적으로 묶어 나가면서 나무 모양(덴드로그램)의 군집 구조를 형성합니다. 20. 강화 학습은 에이전트가 특정 상태(State)에서 어떤 행동(Action)을 취했을 때, 그 결과로 환경으로부터 '상(Reward)' 또는 '벌(Penalty)'을 받습니다. 에이전트의 목표는 장기적으로 '총 보상(상)'을 최대화하는 것이며, 이 과정을 반복하며 어떤 상태에서 어떤 행동을 하는 것이 가장 큰 보상을 받는지(최적의 정책)를 학습하게 됩니다.
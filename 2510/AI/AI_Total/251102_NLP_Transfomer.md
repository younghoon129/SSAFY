# ✍️ 단답형 문제 (10문제)
문제 1. 단어를 표현할 때, '강아지'와 '고양이'의 의미적 '유사성'을 벡터로 표현하지 못하고, 단어 수만큼 '차원'이 커지는 임베딩 방식은 무엇인가?

문제 2. '주변 단어'를 입력받아 '중심 단어'를 예측하는 Word2Vec 모델은 무엇인가? (힌트: CBOW / Skip-gram 중)

문제 3. RNN이 '기울기 소실' 문제로 '장기 의존성' 학습에 한계가 생기는 것을, 'Cell State'와 3개의 게이트를 도입하여 해결한 모델은 무엇인가?

문제 4. '인코더-디코더' 구조의 Seq2Seq 모델이, 입력 문장의 모든 정보를 '마지막 hidden state' 하나에 압축하려 할 때 발생하는 '정보 손실' 문제를 무엇이라고 하는가?

문제 5. Seq2Seq의 '보틀넥' 문제를 해결하기 위해, 디코더가 '입력 시퀀스의 모든 부분'을 '집중(Attention)'하여 참고하는 메커니즘은 무엇인가?

문제 6. 트랜스포머에서 '하나의 시퀀스 내'의 단어들 간 관계를 '병렬적'으로 파악하는, RNN을 대체한 핵심 메커니즘은 무엇인가?

문제 7. 셀프 어텐션은 단어의 '순서' 정보를 잃어버립니다. 이를 해결하기 위해 단어 임베딩에 '위치 벡터'를 더해주는 기법은 무엇인가?

문제 8. 트랜스포머 '디코더'가 '다음 단어'를 생성할 때 '미래(정답)' 단어를 미리 참조하지 못하도록 '가리는' 어텐션 기법은 무엇인가?

문제 9. 트랜스포머의 '인코더' 구조만을 사용하며, 'Masked LM(MLM)'과 'NSP'로 '양방향' 문맥을 학습하는 대표적인 모델은 무엇인가?

문제 10. LLM에게 "질문: ... / 답변: 단계별로 생각하자. 1단계는..."처럼 '추론 과정'을 예시로 주거나 '지시'하여 복잡한 문제 해결 능력을 높이는 프롬프팅 기법은 무엇인가?

# 📜 서술형 문제 (5문제)
문제 11. '원-핫 인코딩'의 치명적인 단점 2가지(차원의 저주, 유사성 표현 불가)를 설명하시오.

문제 12. (⭐️ 중요) '셀프 어텐션(Self-Attention)'의 한계점 3가지와, 각각의 '해결책' 3가지를 짝지어 설명하시오.

문제 13. 트랜스포머의 '인코더(Encoder)'와 '디코더(Decoder)'의 셀프 어텐션 층은 '마스킹(Masking)' 사용 여부에서 결정적인 차이를 보입니다. (1) 둘 중 어디에 마스킹이 사용되는지, (2) 그리고 왜 그곳에 마스킹이 반드시 필요한지 '역할(이해 vs 생성)'과 연관 지어 설명하시오.

문제 14. 'BERT' 모델의 사전 학습 방법 2가지, **(1) 'Masked LM (MLM)'**과 **(2) 'NSP'**가 각각 무엇을 학습하는 것을 목표로 하는지 설명하시오.

문제 15. NLP 모델의 발전 과정을 (1) RNN ➡️ (2) Seq2Seq ➡️ (3) Seq2Seq + Attention ➡️ (4) Transformer(Self-Attention) 순서로 놓고, 각 단계가 '이전 단계의 어떤 한계점'을 '어떻게' 해결했는지 그 흐름을 설명하시오.

# 📝 단답형 / 서술형 예시 답안
단답형 정답

원-핫 인코딩 (One-Hot Encoding)

CBOW (Continuous Bag of Words)

LSTM (Long Short-Term Memory)

보틀넥 문제 (Bottleneck Problem)

어텐션 (Attention)

셀프 어텐션 (Self-Attention)

포지셔널 인코딩 (Positional Encoding)

마스크드 셀프 어텐션 (Masked Self-Attention)

BERT (버트)

Chain-of-Thought (CoT) Prompting (또는 Zero-Shot-CoT)

서술형 정답 (예시) 11.

(1) 차원의 저주: 단어의 개수가 10만 개면 10만 차원의 벡터가 필요합니다. 벡터가 너무 커지고 '희소(Sparse)'해져 계산이 비효율적이고 활용이 어렵습니다.

(2) 유사성 표현 불가: '강아지' [0, 1, 0]와 '고양이' [0, 0, 1]는 벡터 간 내적이 0입니다. 즉, 의미적으로 '비슷한 단어'임에도 '유사성'을 전혀 표현하지 못합니다.

(1) 한계: 순서 정보 부재 / 해결: 포지셔널 인코딩(Positional Encoding) (유사도만 계산하므로 순서를 잃어버려, '위치 벡터'를 더해 순서 정보를 주입합니다.)

(2) 한계: 비선형성 부족 / 해결: 피드 포워드 네트워크(Feed-Forward Network) (셀프 어텐션은 가중합이라 선형에 가까우므로, 'FC+ReLU'로 구성된 FFN을 추가해 '비선형성'을 확장합니다.)

(3) 한계: 미래 참조 문제 / 해결: 마스크드 셀프 어텐션(Masked Self-Attention) ('디코더'가 다음 단어를 생성할 때 '미래(정답)'를 미리 참조하는 것을 '마스킹(Masking)'으로 가립니다.)

(1) 사용 위치: **디코더(Decoder)**에 'Masked' 셀프 어텐션이 사용됩니다. (인코더는 Masking 없음)

(2) 이유: '인코더'의 역할은 문장 전체를 '이해(분석)'하는 것이므로, '양방향' 문맥(과거+미래)을 모두 봐야 해서 Masking을 쓰지 않습니다. 하지만 **'디코더'**의 역할은 '다음 단어를 생성'하는 것입니다. 만약 'I am a' 다음에 올 단어를 예측할 때 '미래' 단어인 'student'를 미리 참조(Cheating)하면 안 되므로, '미래' 부분을 반드시 '가려야(Masking)' 합니다.

(1) Masked LM (MLM): 문장의 일부 단어를 [MASK]로 가린 뒤, '양방향' 문맥(앞/뒤 단어)을 모두 이용하여 '가려진 단어'를 예측(추론)하도록 학습합니다. (깊은 '양방향 문맥 이해' 학습)

(2) NSP: [CLS] 토큰을 사용하여, 두 문장(A, B)이 '실제로 이어지는 문장(IsNext)'인지 '랜덤 문장(NotNext)'인지 이진 분류(판별)하도록 학습합니다. (문장과 문장 '간의 관계' 학습)

(1) RNN: '순차적' 데이터 처리가 가능해졌지만, '기울기 소실' 한계로 '장기 의존성' 학습이 어려웠습니다.

(2) Seq2Seq: RNN(LSTM) 기반 '인코더-디코더'로 '번역'이 가능해졌지만, 입력 정보를 '마지막 hidden state' 하나에 압축하는 '보틀넥(정보 손실)' 한계가 있었습니다.

(3) Seq2Seq + Attention: '어텐션'이 '보틀넥' 문제를 해결(모든 입력 참조)했지만, 'RNN' 기반이라 '순차적'으로만 계산 가능하여 '학습 속도'가 느렸습니다.

(4) Transformer(Self-Attention): 'RNN'을 '완전히 제거'하고 '셀프 어텐션'을 도입하여, **'병렬 처리'(빠른 속도)**와 '장기 의존성'(거리=1)을 동시에 해결했습니다.

이 단원은 현대 AI의 흐름을 이해하는 데 가장 중요합니다. 서술형 12~15번은 꼭 완벽하게 이해